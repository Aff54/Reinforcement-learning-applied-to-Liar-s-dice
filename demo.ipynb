{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7235236-8218-4b5e-9cc1-ba71335c05b8",
   "metadata": {},
   "source": [
    "# Learning Liar's dice with deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f6a94f-6c5f-4e69-9a34-9dce5ea8af01",
   "metadata": {},
   "source": [
    "<ins>Author:</ins> Alexandre Forestier--Foray  (https://github.com/Aff54)  \n",
    "<ins>Date:</ins> 22/01/2026\n",
    "\n",
    "<ins>Note:</ins>\n",
    "- For simplicity, examples are shown with 3 players and 2 dice per player by default. It is possible to increase these numbers but be aware that more players/dice can increase computation time significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6e31b-e876-4a94-9616-a67426301629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Regular Packages ----\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Reinforcement learning.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968789ed-cab0-4102-acdc-719947d50c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Custom packages ----\n",
    "\n",
    "from src.reinforcement_learning import ReplayMemory\n",
    "from src.probability_analysis import plot_situation\n",
    "from src.action_management import get_legal_actions_mask, get_possible_actions\n",
    "from src.deterministic_agents import agent_random, agent_max_probability, agent_min_probability\n",
    "from src.rl_agents import RLAgent\n",
    "from src.game import GameRL\n",
    "from src.performance_analysis.analysis_tools import test_agents, get_player_score\n",
    "from src.performance_analysis.visual_tools import plot_pie_charts\n",
    "from src.rl_agents import RLAgentOnline, EpsilonScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6dc1b6-0ae9-4da0-8c48-e4a7aacb2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU usage.\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"used divice: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4af884-b9bf-4fdb-a2d8-d91f3ac3a29e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Liar's dice game class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ea78a-d07e-4ed3-afb0-5fe7bf31861f",
   "metadata": {},
   "source": [
    "<ins>Note:</ins>\n",
    "- The Game class implements the core mechanics of Liar’s Dice. It is responsible for managing player hands, turn order and player interactions (calling \"liar\" or \"exact\"). It provides an interface for simulating a game.\n",
    "- GameRL subclass extends Game by adding reinforcement learning-specific methods, such as state representation or game set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750d60d-2b41-4874-b056-6dbae7c55f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Game initialisation ----\n",
    "\n",
    "game = GameRL(player_number = 3, max_dice = 2)\n",
    "game.summerize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b66d6-bf92-480e-909d-b940efb37f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting turn info\n",
    "\n",
    "n_dice, last_bet, turn_player, turn_player_hand = game.get_turn_info()\n",
    "\n",
    "print(f\"Turn player: {turn_player}\\n\"\n",
    "      f\"Turn player hand: {turn_player_hand}\\n\"\n",
    "      f\"Last bet: {last_bet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d62da-a0c4-4bc0-af04-dfd709a77608",
   "metadata": {},
   "source": [
    "<ins>Note:</ins> by default, the game starts with $last\\_bet = [1, 0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb2b12-8326-4991-aa22-800cf0d1946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Playing from a player's perspective ----\n",
    "\n",
    "# insert your action here. Format: [quantity, value]\n",
    "# calling \"liar\":[-1, -1] / \"exact\": [0, 0]\n",
    "action = [2, 2]\n",
    "\n",
    "game.make_a_bet(action, verbose = True)\n",
    "\n",
    "# getting turn info\n",
    "n_dice, last_bet, turn_player, turn_player_hand = game.get_turn_info()\n",
    "\n",
    "print(f\"turn player: {turn_player}\\n\"\n",
    "      f\"turn player hand: {turn_player_hand}\\n\"\n",
    "      f\"last bet: {last_bet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692bb1b4-8f16-431b-b897-63cd6aee10ad",
   "metadata": {},
   "source": [
    "<ins>Note:</ins> The Game class does not enforce bet legality. Instead, a legal action generator was incorporated to the agent implementations (deterministic or probabilistic), which filter the action space to ensure that only valid actions are selected. As a result, all agents used in this project are guaranteed to return legal actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549762e-b470-4e89-8aa8-c09a202e0aed",
   "metadata": {},
   "source": [
    "# 2. Deterministic agent functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff1984-cdd7-47a4-aa6a-2b03855ba7c6",
   "metadata": {},
   "source": [
    "<ins>Note:</ins> In order to represent three types of player, for the rl agent to play against, we defined three distinct policies:\n",
    "- A \"survivalist\" policy that maximizes survival: each turn, this agent selects the action (calling \"liar\", \"exact\" or outbidding) with the highest probability of being true. In case of equal probabilities, the most aggressive action (i.e. with highest quantity and/or value ) is chosen.\n",
    "- An \"aggressive\" policy: each turn, selects the action with the lowest probability of being true among those whose probability exceeds a predefined threshold (50% by default). This policy favors high quantity/value bets while maintaining a minimum survival probability.\n",
    "- A \"random\" policy: each turn, returns a random (from uniform distribution) action among legal actions.\n",
    "\n",
    "Action probabilities are computed using a binomial distribution conditioned on the player’s hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d5980-1e8d-4ec8-aece-5e7a829fc617",
   "metadata": {},
   "source": [
    "## 2.1. Visual example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ea25a-7840-4666-8603-44fdb6e376f1",
   "metadata": {},
   "source": [
    "The following cell displays the probability of each outbid to be true condition to the turn player hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b1728-7eeb-407d-b419-368693029b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Displaying deterministic agent's perspective ----\n",
    "\n",
    "# insert total number of dice in game.\n",
    "n_dice = 6\n",
    "\n",
    "# insert last_bet for the agent to play after. Format: [quantity, value].\n",
    "last_bet = [1, 6]\n",
    "\n",
    "# insert player hand. Format: [d_1, d_2, ...] with d_i in {1,..., 6}.\n",
    "turn_player_hand = [2, 3]\n",
    "\n",
    "plot_situation(total_dice = n_dice,\n",
    "               player_hand = turn_player_hand, \n",
    "               last_bet = last_bet)\n",
    "\n",
    "# insert deterministic agent function among { agent_random, agent_max_probability, agent_min_probability }\n",
    "policy_function = agent_max_probability\n",
    "\n",
    "# action selection\n",
    "action = policy_function(last_bet = last_bet, \n",
    "                         total_dice = n_dice,\n",
    "                         player_hand = turn_player_hand,\n",
    "                         verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73da3e-7789-4003-99ee-5c793208034e",
   "metadata": {},
   "source": [
    "## 2.2. Example of strategies playing together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd785de8-6671-4dd6-90be-7e9a6e423375",
   "metadata": {},
   "source": [
    "Use the following cells for making deterministic agent play together. You can add or remove players from players_dict \n",
    "or modify the number of dice players start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672f0e5-7ae4-484a-ae2a-8144c169f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Game set-up ----\n",
    "\n",
    "# dict used for storing player functions. \n",
    "players_dict = {1: agent_max_probability, 2: agent_min_probability, 3: agent_random}\n",
    "\n",
    "# game parameters\n",
    "player_number = len(players_dict)\n",
    "max_dice = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde113b-9d8a-4b5c-b69d-51f579ee4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Game loop ----\n",
    "\n",
    "# Game instantiation\n",
    "game = GameRL(player_number = player_number,\n",
    "              max_dice = max_dice)\n",
    "game.summerize()\n",
    "\n",
    "print(\"---- Game starts ----\")\n",
    "# Playing the game\n",
    "while not game.game_over:\n",
    "\n",
    "    n_dice, last_bet, turn_player, turn_player_hand = game.get_turn_info()\n",
    "\n",
    "    # Action selection by deterministic agent.\n",
    "    print(f\"player {turn_player} is playing with hand {turn_player_hand}\")\n",
    "    action = players_dict[turn_player](last_bet = last_bet, \n",
    "                                       total_dice = n_dice,\n",
    "                                       player_hand = turn_player_hand,\n",
    "                                       verbose = True)\n",
    "\n",
    "    # Taking action.\n",
    "    bet_outcome = game.make_a_bet(bet = action,\n",
    "                                  verbose = True)\n",
    "\n",
    "ranking = game.ranking\n",
    "print(f\"player {ranking[0]} won \\n\"\n",
    "      f\"---- Game ends ----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db8a391-93b3-4c7e-b2e0-a8ef4358d12e",
   "metadata": {},
   "source": [
    "## 2.3 Simulating multiple games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb911bb-a335-4ad2-9000-b56b08a83262",
   "metadata": {},
   "source": [
    "Use the following cells for simulating games with deterministic agents and plot their overall ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed845b93-16e5-440e-9143-fd54609e6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Game set-up ----\n",
    "\n",
    "# dict used for storing player functions\n",
    "players_dict = {1: agent_max_probability, 2: agent_min_probability, 3: agent_random}\n",
    "\n",
    "# game parameters\n",
    "player_number = len(players_dict)\n",
    "max_dice = 2\n",
    "\n",
    "# simulation parameters\n",
    "n_simulation = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e1f384-8f44-4c16-80c2-ca66ee89edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Simulating games ----\n",
    "\n",
    "rankings = np.zeros((n_simulation, player_number))\n",
    "\n",
    "for i in tqdm(range(n_simulation), desc = \"Simulation\"):\n",
    "\n",
    "    # Initialisation.\n",
    "    game = GameRL(player_number = player_number, \n",
    "                  max_dice = max_dice)\n",
    "\n",
    "    # Playing the game\n",
    "    while not game.game_over:\n",
    "    \n",
    "        n_dice, last_bet, turn_player, turn_player_hand = game.get_turn_info()\n",
    "    \n",
    "        # Action selection by deterministic agent.\n",
    "        action = players_dict[turn_player](last_bet = last_bet, \n",
    "                                           total_dice = n_dice,\n",
    "                                           player_hand = turn_player_hand,\n",
    "                                           verbose = False)\n",
    "    \n",
    "        # Taking action.\n",
    "        bet_outcome = game.make_a_bet(bet = action,\n",
    "                                      verbose = False)\n",
    "\n",
    "    # Saving game results\n",
    "    rankings[i, :] = game._ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86cb4b-1bf2-4771-9b49-b0013fdf5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plotting place distribution ----\n",
    "_ = plot_pie_charts(rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba2004-6f49-40f0-8627-28f6e57eca58",
   "metadata": {},
   "source": [
    "# 3. Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f311d17-0afd-4c6d-bef9-f1f2b907ea8d",
   "metadata": {},
   "source": [
    "<ins>Note:</ins>\n",
    "- The training loop is an adaptation of PyTorch's RL tutorial (see https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html);\n",
    "- The algorithm used for training the agent is DDQN (double DQN);\n",
    "- The rl agent is implemented with the RLAgentOnline class. This class stores transitions\n",
    "the agent is going through in a memory buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf23bb-06e2-43e2-ace2-452e3d8a9448",
   "metadata": {},
   "source": [
    "## 3.1 Training an agent with DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f69d611-a10d-46c0-8f01-7c9dfc0043a4",
   "metadata": {},
   "source": [
    "<ins>Note:</ins> In DQN, the network takes the current state as input and outputs a Q-value for **each action**, including illegal ones, as a vector of size (action_number). Thus, we:\n",
    "- encoded actions as integers to index the corresponding network outputs;\n",
    "- applied an action mask to prevent the agent from selecting illegal actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244bfd0-2d41-41ad-8578-2273cb079e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Simulation parameters ----\n",
    "\n",
    "# -- Game parameters --\n",
    "max_dice = 2\n",
    "player_number = 3\n",
    "transition_format = ('state', 'action', 'next_state', 'reward', 'legal_actions_mask', 'game_index')\n",
    "Transition = namedtuple('Transition',\n",
    "                        transition_format)\n",
    "\n",
    "# Getting matching between actions and their index.\n",
    "game = GameRL(player_number = player_number,\n",
    "                       max_dice = max_dice)\n",
    "\n",
    "action_dict = game.get_action_dict()\n",
    "reverse_action_dict = {v: k for k, v in action_dict.items()}\n",
    "# The Q-network input size corresponds to states length.\n",
    "n_states = game.get_n_states()\n",
    "# The Q-network output size is the number of every possible action\n",
    "n_actions = game.get_n_actions()\n",
    "\n",
    "# -- Deep learning parameters --\n",
    "batch_size = 256\n",
    "learning_rate = 3e-3\n",
    "criterion = nn.SmoothL1Loss()\n",
    "metric = nn.L1Loss()\n",
    "weight_decay_rate = 1e-4\n",
    "\n",
    "# -- RL parameters --\n",
    "# reward format: \n",
    "# 0: current player called liar and was right.\n",
    "# -1: current player called liar or exact and was wrong or his bet was challenged and was wrong.\n",
    "# 1: current player called exact and was right. He earned a dice back.\n",
    "# 2: current player outbids\n",
    "# 3: current player won current game\n",
    "# 4: current player outbid, was challenged on and challenger lost a dice\n",
    "# 5: current player made an easy bet, following player called \"exact\" and earned a dice back.\n",
    "# 6: round ended without player losing or earning a dice\n",
    "reward_dict = {0: 1, \n",
    "               -1: -2,\n",
    "               1: 2, \n",
    "               2: 0, \n",
    "               3: 10,\n",
    "               4: 1, \n",
    "               5: -0.5,\n",
    "               6: 0.5} \n",
    "\n",
    "n_training_game = 5000\n",
    "n_test_game = 1000\n",
    "warm_up_duration = 300\n",
    "gamma = 1\n",
    "tau = 0.001\n",
    "eps_max = 1\n",
    "eps_min = 0.2\n",
    "epsilon_scheduler_rate = 1e5/ (3*math.log(10))\n",
    "\n",
    "memory_capacity = 10000\n",
    "memory = ReplayMemory(transitions_tuple = Transition, capacity = memory_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaffdd1-f55d-409b-a176-852c6a221a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- DQN class ----\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_states, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb90751-add2-497d-bac9-88ff0f9a58ca",
   "metadata": {},
   "source": [
    "<ins>Note:</ins> The Q-network architecture used in this project is intentionally simple. More complex architectures could be explored to potentially improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040d122-7e4c-45b6-b89f-f00f9bb7890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(memory, \n",
    "                   criterion = nn.SmoothL1Loss(), \n",
    "                   metric = nn.L1Loss(), \n",
    "                   batch_size = batch_size, \n",
    "                   gamma = gamma):\n",
    "    \n",
    "    if len(memory) < batch_size:\n",
    "        return None, None, None\n",
    "\n",
    "    # -- Randomly sample dataset --\n",
    "    transitions = memory.sample(batch_size = batch_size)\n",
    "    # Convert batch-array of transitions into transition of batch-array.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Making masks for final states. \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, \n",
    "                                            batch.next_state)), \n",
    "                                  device=device, \n",
    "                                  dtype=torch.bool)\n",
    "    non_final_next_states = torch.as_tensor([s \n",
    "                                             for s in batch.next_state \n",
    "                                             if s is not None], \n",
    "                                            dtype=torch.float32).to(device)\n",
    "\n",
    "    # Separating states / actions / rewards / legal action masks\n",
    "    state_batch = torch.as_tensor(batch.state, dtype=torch.float32, device=device)\n",
    "    action_batch = torch.as_tensor(batch.action, dtype=torch.long, device=device)\n",
    "    reward_batch = torch.as_tensor(batch.reward, dtype=torch.float32, device=device)\n",
    "    legal_action_batch = torch.from_numpy(np.stack(batch.legal_actions_mask)).to(device)\n",
    "\n",
    "    # Predicting action Q-values and keeping actions \n",
    "    # that were actually done during simulation.\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "    \n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    # Keeping non final state Q value.\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(non_final_next_states)\n",
    "\n",
    "    # Putting illegal action values to -1e9\n",
    "    legal_mask = legal_action_batch[non_final_mask]\n",
    "    next_actions[~legal_mask] = -1e9\n",
    "    best_next_actions = next_actions.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, best_next_actions).squeeze(1)\n",
    "\n",
    "    # Right side of Bellman's equation : r + gamma*max_a(Q_target(s', a))\n",
    "    expected_state_action_values = reward_batch + (next_state_values * gamma)\n",
    "    expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Computing metric\n",
    "    metric_value = metric(state_action_values, expected_state_action_values).item()\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    # training data\n",
    "    loss_value = loss.item()\n",
    "    avg_prediction_value = state_action_values.mean().detach().cpu().item()\n",
    "\n",
    "    return loss_value, avg_prediction_value, loss_value/avg_prediction_value, metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395dc736-0643-46b0-b535-66fe3a163de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Player definition ----\n",
    "\n",
    "rl_agent = RLAgentOnline(n_states = n_states, \n",
    "                         n_actions = n_actions, \n",
    "                         action_dict = action_dict, \n",
    "                         reward_dict = reward_dict,\n",
    "                         reverse_action_dict = reverse_action_dict,\n",
    "                         transitions_tuple = Transition,\n",
    "                         memory_capacity = memory_capacity,\n",
    "                         DQN_class = DQN)\n",
    "\n",
    "players_dict = {1: agent_max_probability, 2: agent_min_probability, 3: rl_agent}\n",
    "epsilon_rate_scheduler = EpsilonScheduler(eps_max = eps_max, \n",
    "                                         eps_min = eps_min, \n",
    "                                         tau = epsilon_scheduler_rate)\n",
    "\n",
    "# -- Safeguard --\n",
    "# Verify if there are as many players as planned + if there is exactly one RLAgentOnline instance.\n",
    "if len(players_dict) != player_number:\n",
    "    raise ValueError(f\"players_dict has not the same amount of players than given by player_number: players_dict length: {len(players_dict)} VS {player_number}\")\n",
    "rl_agent_index_list = [index for index, agent in players_dict.items() if isinstance(agent, RLAgentOnline)]\n",
    "if len(rl_agent_index_list) != 1:\n",
    "    raise ValueError(f\"The number of rl agents in players_dict is incorrect\")\n",
    "else:\n",
    "    rl_agent_index = rl_agent_index_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cadfaa-eece-4ffc-8092-406e66c4488f",
   "metadata": {},
   "source": [
    "<ins>Note:</ins> Playing games for feeling the rl agent's memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc58550-5aa4-4ef9-aba7-ba1e530c1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Warm-up ----\n",
    "epsilon = epsilon_rate_scheduler.get_epsilon()\n",
    "\n",
    "# Initialize the networks\n",
    "policy_net = DQN(n_states, n_actions).to(device)\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=learning_rate, amsgrad=True, weight_decay = weight_decay_rate)\n",
    "target_net = DQN(n_states, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# -- Warm-up --\n",
    "game_index = 0\n",
    "\n",
    "for i in tqdm(range(warm_up_duration), desc = \"warming-up\", disable = False):\n",
    "    # Initialising game.\n",
    "    game = GameRL(player_number = player_number,\n",
    "                  max_dice = max_dice)\n",
    "    game_index += 1\n",
    "    while not game.game_over:\n",
    "        \n",
    "        n_dice, last_bet, turn_player, turn_player_hand = game.get_turn_info()\n",
    "        last_player = game.active_players[-1]\n",
    "        state = game.get_state()\n",
    "        \n",
    "        # Seperating deterministic functions VS RL agents\n",
    "        if isinstance(players_dict[turn_player], RLAgentOnline): # player_object is an Agent class instance.\n",
    "\n",
    "            action = players_dict[turn_player].epsilon_greedy_select_action(last_bet = last_bet, \n",
    "                                                                            total_dice = n_dice,\n",
    "                                                                            state = state, \n",
    "                                                                            epsilon = epsilon,\n",
    "                                                                            game_index = game_index,\n",
    "                                                                            verbose = False)\n",
    "            # Taking action.\n",
    "            bet_outcome = game.make_a_bet(action,\n",
    "                                          verbose = False)\n",
    "\n",
    "            players_dict[turn_player].receive_outcome(bet_outcome, \n",
    "                                                      game_index = game_index)\n",
    "\n",
    "        else: # player_object is a deterministic function.\n",
    "            action = players_dict[turn_player](last_bet = last_bet,\n",
    "                                               total_dice = n_dice,\n",
    "                                               player_hand = turn_player_hand,\n",
    "                                               verbose = False)\n",
    "            # Taking action.\n",
    "            bet_outcome = game.make_a_bet(action, \n",
    "                                          verbose = False)\n",
    "\n",
    "            # Current bet is a challenge\n",
    "            if bet_outcome != 2:\n",
    "                \n",
    "                # Last player was the rl_agent\n",
    "                if isinstance(players_dict[last_player], RLAgentOnline):\n",
    "                    \n",
    "                    # Current player called tracked player \"liar\" and was right\n",
    "                    if bet_outcome == 0:\n",
    "                        agent_outcome = -1\n",
    "                        # Current player called tracked player \"liar\" or \"exact\" and was wrong\n",
    "                    elif bet_outcome == -1:\n",
    "                        agent_outcome = 4\n",
    "                    # Current player called exact \"exact\" and was right \n",
    "                    # => punishment for a bet too easy to predict but less important than loosing a dice.\n",
    "                    elif bet_outcome == 1:\n",
    "                        agent_outcome = 5\n",
    "\n",
    "                    players_dict[last_player].receive_outcome(agent_outcome, \n",
    "                                                          game_index = game_index)\n",
    "\n",
    "                # Last player was not the rl_agent\n",
    "                else:\n",
    "                    # rl_agent is still active.\n",
    "                    if last_player in game.active_players:\n",
    "                        agent_outcome = 6\n",
    "                    \n",
    "                        players_dict[rl_agent_index].receive_outcome(agent_outcome, \n",
    "                                                        game_index = game_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4db73-51e2-40ab-9d5d-633e2ff14767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training ----\n",
    "\n",
    "# metrics\n",
    "first_place_rate_list = []\n",
    "average_place_list = []\n",
    "train_loss_list = []\n",
    "avg_prediction_value_list = []\n",
    "train_metric_list = []\n",
    "\n",
    "for i in tqdm(range(n_training_game + 1), desc = \"online_training\", disable = False):\n",
    "    # Initialising game.\n",
    "    game = GameRL(player_number = player_number,\n",
    "                  max_dice = max_dice)\n",
    "    game_index += 1\n",
    "\n",
    "    # -- testing network --\n",
    "    if i % 1000 == 0:\n",
    "        rankings, _ = test_agents(max_dice = max_dice, \n",
    "                                players_dict = players_dict, \n",
    "                                n_simulation = n_test_game, \n",
    "                                reward_dict = reward_dict, \n",
    "                                RLAgent_class = RLAgentOnline,\n",
    "                                tqdm_display = False, \n",
    "                                verbose = False)\n",
    "        \n",
    "        first_place_rate = np.count_nonzero(rankings[:, 0] == rl_agent_index)/len(rankings)\n",
    "        average_place = get_player_score(rankings, rl_agent_index)\n",
    "        print(f\"Agent's first place rate: {first_place_rate * 100} % / average place: {average_place}\")\n",
    "\n",
    "        # saving testing metrics\n",
    "        first_place_rate_list.append(first_place_rate)\n",
    "        average_place_list.append(average_place)\n",
    "    \n",
    "    while not game.game_over:\n",
    "        \n",
    "        # -- Playing the game --\n",
    "        n_dice, last_bet, turn_player, turn_player_hand = game.get_turn_info()\n",
    "        last_player = game.active_players[-1]\n",
    "        state = game.get_state()\n",
    "        \n",
    "        # Seperating deterministic functions VS RL agents\n",
    "        if isinstance(players_dict[turn_player], RLAgentOnline): # player_object is an Agent class instance.\n",
    "\n",
    "            epsilon = epsilon_rate_scheduler.get_epsilon()\n",
    "            action = players_dict[turn_player].epsilon_greedy_select_action(last_bet = last_bet, \n",
    "                                                                            total_dice = n_dice,\n",
    "                                                                            state = state, \n",
    "                                                                            epsilon = epsilon,\n",
    "                                                                            game_index = game_index,\n",
    "                                                                            verbose = False)\n",
    "            # Taking action.\n",
    "            bet_outcome = game.make_a_bet(action,\n",
    "                                          verbose = False)\n",
    "\n",
    "            players_dict[turn_player].receive_outcome(bet_outcome, \n",
    "                                                      game_index = game_index)\n",
    "\n",
    "            epsilon_rate_scheduler.step()\n",
    "\n",
    "            # -- Training network --\n",
    "            if epsilon_rate_scheduler.step_count % 10 == 0:\n",
    "\n",
    "                training_step = epsilon_rate_scheduler.step_count\n",
    "                train_loss, avg_prediction_value, loss_avg_prediction_value_rate, metric_value = optimize_model(memory = rl_agent.memory)\n",
    "            \n",
    "                # Updating weights\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key] * tau + target_net_state_dict[key] * (1 - tau)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "                rl_agent.policy_network.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "                # Log training metrics at each step\n",
    "                train_loss_list.append(train_loss)\n",
    "                avg_prediction_value_list.append(avg_prediction_value)\n",
    "                train_metric_list.append(metric_value)\n",
    "                \n",
    "        \n",
    "        else: # player_object is a deterministic function.\n",
    "            action = players_dict[turn_player](last_bet = last_bet,\n",
    "                                   total_dice = n_dice,\n",
    "                                   player_hand = turn_player_hand,\n",
    "                                   verbose = False)\n",
    "            # Taking action.\n",
    "            bet_outcome = game.make_a_bet(action, \n",
    "                                          verbose = False)\n",
    "\n",
    "            # Current bet is a challenge\n",
    "            if bet_outcome != 2:\n",
    "                \n",
    "                # Last player was the rl_agent\n",
    "                if isinstance(players_dict[last_player], RLAgentOnline):\n",
    "                    \n",
    "                    # Current player called tracked player \"liar\" and was right\n",
    "                    if bet_outcome == 0:\n",
    "                        agent_outcome = -1\n",
    "                        # Current player called tracked player \"liar\" or \"exact\" and was wrong\n",
    "                    elif bet_outcome == -1:\n",
    "                        agent_outcome = 4\n",
    "                    # Current player called exact \"exact\" and was right \n",
    "                    # => punishment for a bet too easy to predict but less important than loosing a dice.\n",
    "                    elif bet_outcome == 1:\n",
    "                        agent_outcome = 5\n",
    "\n",
    "                    players_dict[last_player].receive_outcome(agent_outcome, \n",
    "                                                          game_index = game_index)\n",
    "\n",
    "                # Last player was not the rl_agent\n",
    "                else:\n",
    "                    # rl_agent is still active.\n",
    "                    if last_player in game.active_players:\n",
    "                        agent_outcome = 6\n",
    "                    \n",
    "                        players_dict[rl_agent_index].receive_outcome(agent_outcome, \n",
    "                                                        game_index = game_index)\n",
    "\n",
    "        # Ending game faster if rl agent lost\n",
    "        if rl_agent_index not in game.active_players:\n",
    "            break\n",
    "\n",
    "    \n",
    "# -- Plotting last rankings array as pie chart --\n",
    "_ = plot_pie_charts(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573574f-eb52-4175-b231-56ca050cc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plotting agent's performance ----\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# -- epsilon over environment steps --\n",
    "ax[0].plot(epsilon_rate_scheduler.epsilon_history)\n",
    "ax[0].set_xlabel(\"Agent actions\")\n",
    "ax[0].set_ylabel(\"Epsilon\")\n",
    "ax[0].set_title(\"Exploration rate over training\")\n",
    "ax[0].set_ylim(0, 1.1)\n",
    "\n",
    "# -- first-place rate over training --\n",
    "ax[1].plot(np.arange(0, len(first_place_rate_list)*1000, 1000), first_place_rate_list)\n",
    "ax[1].set_xlabel(\"Played games\")\n",
    "ax[1].set_ylabel(\"First-place rate\")\n",
    "ax[1].set_title(\"RL agent first-place rate\")\n",
    "ax[1].set_ylim(-0.1, 1.1)\n",
    "\n",
    "# -- average placement over training --\n",
    "ax[2].plot(np.arange(0, len(average_place_list)*1000, 1000), average_place_list)\n",
    "ax[2].set_xlabel(\"Played games\")\n",
    "ax[2].set_ylabel(\"Average placement\")\n",
    "ax[2].set_title(\"RL agent average placement\")\n",
    "ax[2].set_ylim(0.9, player_number + 0.1)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a9eec-1c3a-41b5-b75b-102d0a4ab86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plotting agent's performance ----\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# -- epsilon over environment steps --\n",
    "ax[0].plot(np.arange(0, len(train_loss_list)*10, 10), train_loss_list)\n",
    "ax[0].set_xlabel(\"Training steps (Agent actions)\")\n",
    "ax[0].set_ylabel(\"Loss value\")\n",
    "ax[0].set_title(\"Training loss (huber loss)\")\n",
    "\n",
    "# -- first-place rate over training --\n",
    "ax[1].plot(np.arange(0, len(avg_prediction_value_list)*10, 10), avg_prediction_value_list)\n",
    "ax[1].set_xlabel(\"Training steps (Agent actions)\")\n",
    "ax[1].set_ylabel(\"Average prediction value\")\n",
    "ax[1].set_title(\"Q-network average prediction value\")\n",
    "\n",
    "# -- average placement over training --\n",
    "ax[2].plot(np.arange(0, len(train_metric_list)*10, 10), train_metric_list)\n",
    "ax[2].set_xlabel(\"Training steps (Agent actions)\")\n",
    "ax[2].set_ylabel(\"Metric value\")\n",
    "ax[2].set_title(\"Training metric (L1 norm)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c67b4-af32-4596-b433-b30037c83b03",
   "metadata": {},
   "source": [
    "## 3.2 Making RL agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388a653-9769-47aa-8cf1-73f63a903812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Game loop ----\n",
    "\n",
    "# Game instantiation\n",
    "game = GameRL(player_number = player_number,\n",
    "              max_dice = max_dice)\n",
    "game.summerize()\n",
    "\n",
    "print(\"---- Game starts ----\")\n",
    "# Playing the game\n",
    "while not game.game_over:\n",
    "\n",
    "    n_dice, last_bet, turn_player, turn_player_hand = game.get_turn_info()\n",
    "\n",
    "    # Action selection by deterministic agent.\n",
    "    print(f\"player {turn_player} is playing with hand {turn_player_hand}\")\n",
    "\n",
    "    # Seperating deterministic functions VS RL agents\n",
    "    if isinstance(players_dict[turn_player], RLAgentOnline): # player_object is an Agent class instance.\n",
    "\n",
    "        state = game.get_state()\n",
    "        action = players_dict[turn_player].select_action(last_bet = last_bet, \n",
    "                                                         total_dice = n_dice, \n",
    "                                                         state = state)\n",
    "        print(f\"RL agent plays {action}\")\n",
    "\n",
    "    else:\n",
    "                    \n",
    "        action = players_dict[turn_player](last_bet = last_bet, \n",
    "                                           total_dice = n_dice,\n",
    "                                           player_hand = turn_player_hand,\n",
    "                                           verbose = True)\n",
    "\n",
    "    # Taking action.\n",
    "    bet_outcome = game.make_a_bet(bet = action,\n",
    "                                  verbose = True)\n",
    "\n",
    "ranking = game.ranking\n",
    "print(f\"player {ranking[0]} won \\n\"\n",
    "      f\"---- Game ends ----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
